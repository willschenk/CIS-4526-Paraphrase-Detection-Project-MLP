{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9434c372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train With Label Shape: (7801, 4)\n",
      "Dev With Label Shape: (4000, 4)\n",
      "Test Without Label Shape: (4000, 3)\n"
     ]
    }
   ],
   "source": [
    "#### Import Data #### \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn import preprocessing \n",
    "\n",
    "# Import training set \n",
    "with open('train_with_label.txt') as file:\n",
    "    train_buffer = file.readlines()\n",
    "    train = pd.DataFrame([row.split('\\t') for row in train_buffer], columns=['ID', 'sentence1', 'sentence2', 'groundTruth'])\n",
    "    train.groundTruth = train.groundTruth.apply(lambda x: int(x.rstrip()))\n",
    "\n",
    "# Import dev set \n",
    "with open('dev_with_label.txt') as file:\n",
    "    dev_buffer = file.readlines()\n",
    "    dev = pd.DataFrame([row.split('\\t') for row in dev_buffer], columns=['ID', 'sentence1', 'sentence2', 'groundTruth'])\n",
    "    dev.groundTruth = dev.groundTruth.apply(lambda x: int(x.rstrip()))\n",
    "\n",
    "# Import testing set \n",
    "with open('test_without_label.txt') as file:\n",
    "    test_buffer = file.readlines()\n",
    "    test = pd.DataFrame([row.split('\\t') for row in test_buffer], columns=['ID', 'sentence1', 'sentence2']) \n",
    "\n",
    "    \n",
    "# Print out shapes \n",
    "print(\"Train With Label Shape: \" + str(train.shape)) \n",
    "print(\"Dev With Label Shape: \" + str(dev.shape)) \n",
    "print(\"Test Without Label Shape: \" + str(test.shape)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e157d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate Features #### \n",
    "\n",
    "import nltk \n",
    "from Levenshtein import distance as lev \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from nltk.corpus import wordnet \n",
    "from nltk.translate.bleu_score import SmoothingFunction \n",
    "\n",
    "\n",
    "def commonSynonyms(wordSet1, wordSet2): \n",
    "    commonSynonyms = 0 \n",
    "    for word1 in wordSet1: # For every word in set 1  \n",
    "        synonyms = [] # Get synonyms for this word\n",
    "        for syn in wordnet.synsets(word1): \n",
    "            for lem in syn.lemmas(): \n",
    "                synonyms.append(lem.name()) \n",
    "        # For each word in set 2 \n",
    "        for word2 in wordSet2: \n",
    "            if word2 in synonyms: \n",
    "                commonSynonyms = commonSynonyms + 1 \n",
    "    return commonSynonyms \n",
    "\n",
    "# Function: Generate features provided two sentences \n",
    "def featureCreation(frame): # Pass in the information \n",
    "    df = pd.DataFrame(frame) # Convert to dataFrame \n",
    "     \n",
    "    # Vectorize each sentence and clean each element \n",
    "    token = WhitespaceTokenizer() \n",
    "    df['words1'] = df['sentence1'].str.replace('[^\\'0-9A-Za-z ]+', '').str.lower().str.replace(\" '\", \"'\").str.replace(\"  \", \" \").str.split(' ') \n",
    "    df['words2'] = df['sentence2'].str.replace('[^\\'0-9A-Za-z ]+', '').str.lower().str.replace(\" '\", \"'\").str.replace(\"  \", \" \").str.split(' ') \n",
    "    df['words1'] = df.apply(lambda x: (list(filter(lambda item: item, x['words1']))), axis = 1) \n",
    "    df['words2'] = df.apply(lambda x: (list(filter(lambda item: item, x['words2']))), axis = 1) \n",
    "    \n",
    "    # Lemmatize each word \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    df['words1'] = df.apply(lambda x: ([lemmatizer.lemmatize(word) for word in x['words1']]), axis = 1) \n",
    "    df['words2'] = df.apply(lambda x: ([lemmatizer.lemmatize(word) for word in x['words2']]), axis = 1) \n",
    "\n",
    "    # Word stemmer: \n",
    "    ps = PorterStemmer() # Also try this later \n",
    "    snow_stemmer = SnowballStemmer(language='english') \n",
    "    df['words1'] = df.apply(lambda x: ([snow_stemmer.stem(word) for word in x['words1']]), axis = 1) \n",
    "    df['words2'] = df.apply(lambda x: ([snow_stemmer.stem(word) for word in x['words2']]), axis = 1) \n",
    " \n",
    "\n",
    "    df['CommonSynonymCount'] = df.apply(lambda x: commonSynonyms(x['words1'], x['words2']), axis = 1)\n",
    "    df['WordsInCommon'] = df.apply(lambda x: (len(np.intersect1d(x['words1'],x['words2']))), axis = 1) \n",
    "    \n",
    "    # Find the number of words in common, then divide by the average number of words between the two sentences \n",
    "    df['WordsInCommon/AverageLen'] = df.apply(lambda x: (len(np.intersect1d(x['words1'],x['words2']))) / (len(x['words1']) + len(x['words2']))/2, axis = 1)\n",
    "    \n",
    "    # Obtain the difference in length between the two sentences \n",
    "    df['differenceInLength'] = df.apply(lambda x: abs(len(x['words1']) - len(x['words2'])), axis = 1)\n",
    "\n",
    "    # BiLingual Evaluation Understudy Score \n",
    "    df['Bleu Score'] = df.apply(lambda x: nltk.translate.bleu_score.sentence_bleu([x['words1']], x['words2'], auto_reweigh = True), axis = 1) \n",
    "\n",
    "    # Levenshtein Score \n",
    "    df['Levenshtein'] = 0\n",
    "    for i in range(len(df)): \n",
    "        df['Levenshtein'].iloc[i] = lev(df['words1'].iloc[i], df['words2'].iloc[i]) / ((len(df['words1'].iloc[i]) + len(df['words2'].iloc[i])) / 2) \n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395458ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/drn2hrr90kv396cxlgjbbryw0000gn/T/ipykernel_24764/3571236682.py:33: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['words1'] = df['sentence1'].str.replace('[^\\'0-9A-Za-z ]+', '').str.lower().str.replace(\" '\", \"'\").str.replace(\"  \", \" \").str.split(' ')\n",
      "/var/folders/jq/drn2hrr90kv396cxlgjbbryw0000gn/T/ipykernel_24764/3571236682.py:34: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['words2'] = df['sentence2'].str.replace('[^\\'0-9A-Za-z ]+', '').str.lower().str.replace(\" '\", \"'\").str.replace(\"  \", \" \").str.split(' ')\n",
      "/Users/christopherschenk/opt/anaconda3/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/christopherschenk/opt/anaconda3/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/christopherschenk/opt/anaconda3/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/christopherschenk/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeros in the training set: 5901\n",
      "New number of zeros in the training set: 5901\n",
      "New number of ones in the training set: 5901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherschenk/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3699: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[iloc] = igetitem(value, i)\n",
      "/Users/christopherschenk/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3699: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[iloc] = igetitem(value, i)\n",
      "/Users/christopherschenk/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3699: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[iloc] = igetitem(value, i)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn import linear_model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics \n",
    "from sklearn.utils import resample \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import accuracy_score \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance \n",
    "from gensim.models import Word2Vec \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "# Generate the features the datasets: \n",
    "train = featureCreation(train) \n",
    "dev = featureCreation(dev) \n",
    "test = featureCreation(test) \n",
    "\n",
    "# # Upsampling: \n",
    "print(\"Number of zeros in the training set: \" + str((train['groundTruth'] == 0).sum())) \n",
    "\n",
    "# # Thus, an upsampling must be preformed: \n",
    "zeros = train[train['groundTruth'] == 0] \n",
    "ones = train[train['groundTruth'] == 1] \n",
    "ones = resample(ones, replace = True, n_samples = len(zeros), random_state = 1)\n",
    "train = pd.concat([zeros, ones]) \n",
    "train = train.reset_index() \n",
    "\n",
    "print(\"New number of zeros in the training set: \" + str((train['groundTruth'] == 0).sum())) \n",
    "print(\"New number of ones in the training set: \" + str((train['groundTruth'] == 1).sum())) \n",
    "\n",
    "# Set features that will be used in the model:  \n",
    "X_train = train[['WordsInCommon/AverageLen', 'Bleu Score', 'Levenshtein', 'differenceInLength', 'WordsInCommon/AverageLen', 'CommonSynonymCount', 'WordsInCommon']] # Training X \n",
    "X_dev = dev[['WordsInCommon/AverageLen', 'Bleu Score', 'Levenshtein', 'differenceInLength', 'WordsInCommon/AverageLen', 'CommonSynonymCount', 'WordsInCommon']] # Dev X \n",
    "y_train = train['groundTruth'] # Training Y \n",
    "y_dev = dev['groundTruth'] # Dev Y \n",
    "# What you will run the program on: \n",
    "X_test = test[['WordsInCommon/AverageLen', 'Bleu Score', 'Levenshtein', 'differenceInLength', 'WordsInCommon/AverageLen', 'CommonSynonymCount', 'WordsInCommon']] # Final testing X \n",
    "\n",
    "# Normalize the data: \n",
    "scaler = MinMaxScaler() \n",
    "X_train[['WordsInCommon/AverageLen', 'Bleu Score', 'Levenshtein', 'differenceInLength', 'WordsInCommon/AverageLen', 'CommonSynonymCount', 'WordsInCommon']] = scaler.fit_transform(X_train[['WordsInCommon/AverageLen', 'Bleu Score', 'Levenshtein', 'differenceInLength', 'WordsInCommon/AverageLen', 'CommonSynonymCount', 'WordsInCommon']]) \n",
    "X_dev[['WordsInCommon/AverageLen', 'Bleu Score', 'Levenshtein', 'differenceInLength', 'WordsInCommon/AverageLen', 'CommonSynonymCount', 'WordsInCommon']] = scaler.transform(X_dev[['WordsInCommon/AverageLen', 'Bleu Score', 'Levenshtein', 'differenceInLength', 'WordsInCommon/AverageLen', 'CommonSynonymCount', 'WordsInCommon']]) \n",
    "X_test[['WordsInCommon/AverageLen', 'Bleu Score', 'Levenshtein', 'differenceInLength', 'WordsInCommon/AverageLen', 'CommonSynonymCount', 'WordsInCommon']] = scaler.transform(X_test[['WordsInCommon/AverageLen', 'Bleu Score', 'Levenshtein', 'differenceInLength', 'WordsInCommon/AverageLen', 'CommonSynonymCount', 'WordsInCommon']]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e78b13b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8169491525423729\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Apply MLP Classifier \n",
    "\n",
    "model = MLPClassifier(random_state = 1, max_iter = 400, learning_rate = 'adaptive').fit(X_train, y_train) \n",
    "\n",
    "prediction = model.predict(X_dev) \n",
    "print(f1_score(prediction, y_dev)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e975f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Submission: \n",
    "y_pred = model.predict(X_test) \n",
    "test['y_pred'] = y_pred.tolist() \n",
    "submission = test[['ID', 'y_pred']] \n",
    "submission.to_csv('Will_Schenk_test_result.txt', sep ='\\t', header = False, index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
